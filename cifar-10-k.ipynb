{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader, Subset\nimport pandas as pd\nfrom PIL import Image\n\n# Check if GPU is available\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Create directories for the dataset\nbase_dir = \"/kaggle/working/attack_prediction_dataset/cifar10\"\nos.makedirs(os.path.join(base_dir, \"train/clean\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"train/fgsm\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"train/bim\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"train/cw\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"val/clean\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"val/fgsm\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"val/bim\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"val/cw\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"test/clean\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"test/fgsm\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"test/bim\"), exist_ok=True)\nos.makedirs(os.path.join(base_dir, \"test/cw\"), exist_ok=True)\n\n# Load CIFAR-10 dataset\ntransform = transforms.Compose([transforms.ToTensor()])\ntrain_dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=transform)\ntest_dataset = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=transform)\n\n# Subsample the datasets to limit to 5,000 images\ntrain_size = 4000  # Number of training images\nval_size = 500     # Number of validation images\ntest_size = 500    # Number of test images\n\n# Subsample the training dataset\ntrain_dataset = Subset(train_dataset, range(train_size))\n\n# Split the training set into training and validation\ntrain_size_final = int(0.8 * train_size)  # 80% of train_size for training\nval_size_final = train_size - train_size_final  # Remaining 20% for validation\ntrain_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size_final, val_size_final])\n\n# Subsample the test dataset\ntest_dataset = Subset(test_dataset, range(test_size))\n\n# Print the sizes of the datasets\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\nprint(f\"Test set size: {len(test_dataset)}\")\n\n# Define a simple CNN model for CIFAR-10\nclass SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # 3 input channels for RGB\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.fc1 = nn.Linear(64 * 8 * 8, 128)  # CIFAR-10 images are 32x32, so after 2 max-pooling layers: 32/2/2 = 8x8\n        self.fc2 = nn.Linear(128, 10)  # 10 classes in CIFAR-10\n\n    def forward(self, x):\n        x = torch.relu(self.conv1(x))\n        x = torch.max_pool2d(x, 2)\n        x = torch.relu(self.conv2(x))\n        x = torch.max_pool2d(x, 2)\n        x = x.view(x.size(0), -1)\n        x = torch.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n\n# Train the model\ndef train_model(model, train_loader, val_loader, epochs=5):\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n\n    for epoch in range(epochs):\n        model.train()\n        for images, labels in train_loader:\n            images, labels = images.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n\n        # Validate\n        model.eval()\n        val_loss = 0\n        correct = 0\n        with torch.no_grad():\n            for images, labels in val_loader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                val_loss += criterion(outputs, labels).item()\n                _, predicted = torch.max(outputs, 1)\n                correct += (predicted == labels).sum().item()\n\n        print(f\"Epoch {epoch+1}/{epochs}, Loss: {loss.item():.4f}, Val Accuracy: {correct/len(val_loader.dataset):.4f}\")\n\n# Initialize model and data loaders\nmodel = SimpleCNN().to(device)\ntrain_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n\n# Train the model\ntrain_model(model, train_loader, val_loader, epochs=5)\n\n# Save the trained model\ntorch.save(model.state_dict(), \"cifar10_cnn.pth\")\nprint(\"Model saved as cifar10_cnn.pth\")\n\n# Load the trained model\nmodel = SimpleCNN().to(device)\nmodel.load_state_dict(torch.load(\"cifar10_cnn.pth\", map_location=device, weights_only=True))  # Fix: Use weights_only=True\nmodel.eval()\n\n# FGSM Attack\ndef fgsm_attack(image, epsilon, data_grad):\n    sign_data_grad = data_grad.sign()  # Get the sign of the gradient\n    perturbed_image = image + epsilon * sign_data_grad  # Add perturbation\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)  # Clip to valid pixel range\n    return perturbed_image\n\n# BIM Attack (Iterative FGSM)\ndef bim_attack(image, epsilon, alpha, iterations, model, target_label):\n    perturbed_image = image.clone().detach().requires_grad_(True)  # Fix: Create a new tensor with requires_grad\n    for _ in range(iterations):\n        output = model(perturbed_image)\n        loss = nn.CrossEntropyLoss()(output, target_label)\n        model.zero_grad()\n        loss.backward()\n        data_grad = perturbed_image.grad.data\n        perturbed_image = perturbed_image + alpha * data_grad.sign()\n        perturbed_image = torch.clamp(perturbed_image, image - epsilon, image + epsilon)\n        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n        perturbed_image = perturbed_image.detach().requires_grad_(True)  # Fix: Re-enable requires_grad\n    return perturbed_image\n\n# Carlini & Wagner Attack (L2 norm) - Fixed\ndef cw_attack(image, target_label, model, confidence=10, learning_rate=0.01, max_iterations=100):\n    # Ensure target_label is a tensor\n    target_label = torch.tensor([target_label], device=device)\n    \n    # Define the perturbation variable\n    delta = torch.zeros_like(image, requires_grad=True).to(device)\n    optimizer = optim.Adam([delta], lr=learning_rate)\n    \n    for _ in range(max_iterations):\n        perturbed_image = image + delta\n        perturbed_image = torch.clamp(perturbed_image, 0, 1)\n        output = model(perturbed_image)\n        \n        # Get the correct logit and the maximum logit of other classes\n        correct_logit = output[:, target_label]\n        \n        # Mask the correct logit to find the maximum of other logits\n        other_logits = output.clone()\n        other_logits[:, target_label] = -float('inf')  # Mask the correct logit\n        max_other_logit = other_logits.max(dim=1).values  # Find the maximum of other logits\n        \n        # Compute the loss\n        loss = torch.max(correct_logit - max_other_logit + confidence, torch.tensor(0.0, device=device))\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    perturbed_image = image + delta\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    return perturbed_image\n\n# Generate adversarial examples\ndef generate_adversarial_examples(dataset, model, epsilon, output_dir, metadata, attack_type):\n    dataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n    for i, (image, label) in enumerate(dataloader):\n        image, label = image.to(device), label.to(device)\n        \n        # Skip clean data generation (already done)\n        clean_path = os.path.join(output_dir, \"clean\", f\"{i}.png\")\n        if not os.path.exists(clean_path):\n            clean_image = image.squeeze().permute(1, 2, 0).cpu().detach().numpy() * 255  # Convert to HWC format\n            Image.fromarray(clean_image.astype(np.uint8)).save(clean_path)\n            metadata.append({\n                \"image_path\": clean_path,\n                \"attack_type\": \"clean\",\n                \"attack_parameters\": None,\n                \"original_label\": label.item(),\n                \"attacked_label\": label.item()\n            })\n\n        # Generate adversarial example\n        if attack_type == \"fgsm\":\n            image.requires_grad = True\n            output = model(image)\n            loss = nn.CrossEntropyLoss()(output, label)\n            model.zero_grad()\n            loss.backward()\n            data_grad = image.grad.data\n            perturbed_image = fgsm_attack(image, epsilon, data_grad)\n        elif attack_type == \"bim\":\n            perturbed_image = bim_attack(image, epsilon, alpha=0.01, iterations=10, model=model, target_label=label)\n        elif attack_type == \"cw\":\n            perturbed_image = cw_attack(image, label, model)\n\n        # Save adversarial image\n        adv_path = os.path.join(output_dir, attack_type, f\"{i}.png\")\n        adv_image = perturbed_image.squeeze().permute(1, 2, 0).cpu().detach().numpy() * 255  # Convert to HWC format\n        Image.fromarray(adv_image.astype(np.uint8)).save(adv_path)\n\n        # Add metadata\n        metadata.append({\n            \"image_path\": adv_path,\n            \"attack_type\": attack_type,\n            \"attack_parameters\": f\"epsilon={epsilon}\" if attack_type in [\"fgsm\", \"bim\"] else \"confidence=10\",\n            \"original_label\": label.item(),\n            \"attacked_label\": label.item()\n        })\n\n# Generate adversarial examples for train, val, and test sets\nepsilon = 0.1  # Attack strength for FGSM and BIM\nmetadata_train, metadata_val, metadata_test = [], [], []\n\n# FGSM\ngenerate_adversarial_examples(train_dataset, model, epsilon, os.path.join(base_dir, \"train\"), metadata_train, \"fgsm\")\ngenerate_adversarial_examples(val_dataset, model, epsilon, os.path.join(base_dir, \"val\"), metadata_val, \"fgsm\")\ngenerate_adversarial_examples(test_dataset, model, epsilon, os.path.join(base_dir, \"test\"), metadata_test, \"fgsm\")\n\n# BIM\ngenerate_adversarial_examples(train_dataset, model, epsilon, os.path.join(base_dir, \"train\"), metadata_train, \"bim\")\ngenerate_adversarial_examples(val_dataset, model, epsilon, os.path.join(base_dir, \"val\"), metadata_val, \"bim\")\ngenerate_adversarial_examples(test_dataset, model, epsilon, os.path.join(base_dir, \"test\"), metadata_test, \"bim\")\n\n# C&W\ngenerate_adversarial_examples(train_dataset, model, epsilon, os.path.join(base_dir, \"train\"), metadata_train, \"cw\")\ngenerate_adversarial_examples(val_dataset, model, epsilon, os.path.join(base_dir, \"val\"), metadata_val, \"cw\")\ngenerate_adversarial_examples(test_dataset, model, epsilon, os.path.join(base_dir, \"test\"), metadata_test, \"cw\")\n\n# Save metadata to CSV files\npd.DataFrame(metadata_train).to_csv(os.path.join(base_dir, \"train/metadata_train.csv\"), index=False)\npd.DataFrame(metadata_val).to_csv(os.path.join(base_dir, \"val/metadata_val.csv\"), index=False)\npd.DataFrame(metadata_test).to_csv(os.path.join(base_dir, \"test/metadata_test.csv\"), index=False)\n\nprint(\"Adversarial dataset created successfully!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:10:35.570939Z","iopub.execute_input":"2025-03-21T19:10:35.571247Z","iopub.status.idle":"2025-03-21T19:33:19.664000Z","shell.execute_reply.started":"2025-03-21T19:10:35.571223Z","shell.execute_reply":"2025-03-21T19:33:19.662906Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nDownloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 170M/170M [00:05<00:00, 33.5MB/s] \n","output_type":"stream"},{"name":"stdout","text":"Extracting ./data/cifar-10-python.tar.gz to ./data\nFiles already downloaded and verified\nTraining set size: 3200\nValidation set size: 800\nTest set size: 500\nEpoch 1/5, Loss: 2.0384, Val Accuracy: 0.2712\nEpoch 2/5, Loss: 1.7342, Val Accuracy: 0.3237\nEpoch 3/5, Loss: 1.7120, Val Accuracy: 0.3987\nEpoch 4/5, Loss: 1.4940, Val Accuracy: 0.4000\nEpoch 5/5, Loss: 1.3251, Val Accuracy: 0.4275\nModel saved as cifar10_cnn.pth\nAdversarial dataset created successfully!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\n\n# Compress the dataset folder\nshutil.make_archive(\"/kaggle/working/attack_prediction_dataset\", 'zip', \"/kaggle/working/attack_prediction_dataset\")\nprint(\"Dataset compressed successfully!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-21T19:35:38.557039Z","iopub.execute_input":"2025-03-21T19:35:38.557369Z","iopub.status.idle":"2025-03-21T19:35:42.251757Z","shell.execute_reply.started":"2025-03-21T19:35:38.557345Z","shell.execute_reply":"2025-03-21T19:35:42.250695Z"}},"outputs":[{"name":"stdout","text":"Dataset compressed successfully!\n","output_type":"stream"}],"execution_count":2}]}